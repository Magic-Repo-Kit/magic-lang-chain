{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义提示词模板\n",
    "RESPONSE_TEPLATE = \"\"\"\\\n",
    "你作为一名专业程序员和问题解决者，负责回答关于Langchain的任何问题并且严格遵守下面的8个准则。\\\n",
    "1.基于所提供搜索结果（URL和context）生成一份全面并且信息丰富的回答。\\\n",
    "2.你必须只使用所提供的搜索中的内容，将搜索结果合并成一个连贯的答案，不要重复内容，以公正无偏见的新闻报道风格撰写。\\\n",
    "3.使用[${{number}}]格式进行引用，只引用最相关的搜索结果来准确的回答问题。\\\n",
    "4.将引用放在引用放在引用它们的句子或段落末尾，而不是全部放在最后。\\\n",
    "5.如果不同的结果指的是同名的不同实体，为每个实体编写单独的答案。\\\n",
    "6.使用项目符号列表以提高可读性，将引用放在适当位置，而不是全部放在最后。\\\n",
    "7.如果上下文中没有与问题相关的信息，请说“嗯，我不确定。”不要试图编造答案。\\\n",
    "8.下面'context'的HTML块之间的任何内容都来自知识库，不是与用户的对话内容。\\\n",
    "\n",
    "<context>\n",
    "    {context} \n",
    "<context/>\n",
    "\n",
    "记住：如果上下文中没有相关信息，只需说“嗯，我不确定。”不要试图编造答案。位于上述'context'HTML块之间的任何内容都是从知识库中检索的，不是与用户的对话内容。\\\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_TEMPLATE = \"\"\"\n",
    "鉴于以下对话和后续问题，请重新表述后续问题，使其成为一个独立的问题。\\\n",
    "历史对话:\n",
    "{chat_history}\n",
    "后续问题输入:\n",
    "{question}\n",
    "独立的问题:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-gRbZ9FJz2E7c7mwO5JOvp2u2rtoWoAbg12CxDy3Y25eLeDvd\"\n",
    "os.environ['OPENAI_API_BASE'] = \"https://api.chatanywhere.tech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pydantic import BaseModel\n",
    "from pydantic.dataclasses import dataclass\n",
    "from typing import Dict, List, Optional,Sequence\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.schema.runnable import Runnable,RunnableBranch,RunnableLambda,RunnableMap\n",
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "\n",
    "# 定义聊天类\n",
    "@dataclass(config=Config)\n",
    "class ChatRequest(BaseModel):\n",
    "  chat_history: Optional[List[Dict[str,str]]]\n",
    "  question: str\n",
    "  \n",
    "\n",
    "# 定义向量解析模型\n",
    "def get_embeddings_model() -> Embeddings:\n",
    "  return OpenAIEmbeddings(\n",
    "  openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "  openai_api_base=os.environ['OPENAI_API_BASE']+ \"/v1\",\n",
    ")\n",
    "\n",
    "# 定义文档解析器\n",
    "def get_retriever() -> BaseRetriever:\n",
    "  els_client = ElasticsearchStore(\n",
    "    es_url=\"http://154.204.60.125:9200\",\n",
    "    index_name=\"text_1\",\n",
    "    embedding=get_embeddings_model(),\n",
    "  )\n",
    "  return els_client.as_retriever()\n",
    "\n",
    "# 定义chain(定义路由是否依赖历史)\n",
    "def create_retriever_chain(llm: BaseLanguageModel,retriever: BaseRetriever) -> Runnable:\n",
    "  # 问题压缩\n",
    "  CONDEN_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "  condense_question_chain = (\n",
    "    CONDEN_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "  ).with_config(\n",
    "    run_name=\"CondenseQuestion\",\n",
    "  )\n",
    "  # 问题压缩与检索\n",
    "  conversation_chain= condense_question_chain | retriever\n",
    "  return RunnableBranch(\n",
    "      (\n",
    "          RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "          ),\n",
    "          conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "      ),\n",
    "      (\n",
    "          RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "            run_name=\"Itemgetter:question\"\n",
    "          )\n",
    "          | retriever\n",
    "      ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    "  ).with_config(run_name=\"RouteDependingOnChatHistory\")\n",
    "  \n",
    "# 格式化文档\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "  formatted_docs = []\n",
    "  for i,doc in enumerate(docs):\n",
    "    doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "    formatted_docs.append(doc_string)\n",
    "  return \"\\n\".join(formatted_docs)\n",
    "\n",
    "# 序列化历史记录\n",
    "def serialize_history(request: ChatRequest):\n",
    "  chat_history = request[\"chat_history\"] or []\n",
    "  converted_chat_history = []\n",
    "  for message in chat_history:\n",
    "    if message.get(\"human\") is not None:\n",
    "      converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "    if message.get(\"ai\") is not None:\n",
    "      converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "  return converted_chat_history\n",
    "\n",
    "# 创建链函数\n",
    "def create_chain(\n",
    "    llm: BaseLanguageModel,\n",
    "    retriever: BaseRetriever,\n",
    ") -> Runnable:\n",
    "  retriever_chain = create_retriever_chain(llm=llm,retriever=retriever).with_config(run_name=\"FindDocs\")\n",
    "  _context = RunnableMap(\n",
    "    {\n",
    "      \"context\": retriever_chain | format_docs,\n",
    "      \"question\": itemgetter(\"question\"),\n",
    "      \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "  ).with_config(run_name=\"RetrieveDocs\")\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\",RESPONSE_TEPLATE),\n",
    "      MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "      (\"human\",\"{question}\")\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  #答案生成器\n",
    "  response_synthesizer = (prompt | llm | StrOutputParser()).with_config(\n",
    "    run_name=\"GenerateResponse\",\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    {\n",
    "      \"question\": RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "        run_name=\"Itemgetter:question\"\n",
    "      ),\n",
    "      \"chat_history\": RunnableLambda(serialize_history).with_config(\n",
    "        run_name=\"SerializeHistory\"\n",
    "      ),\n",
    "    } | _context | response_synthesizer\n",
    "  )\n",
    "\n",
    "# 定义大语言模型\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "  openai_api_base=os.environ['OPENAI_API_BASE'],\n",
    ")\n",
    "\n",
    "# 获取文档解析器\n",
    "retriever = get_retriever()\n",
    "\n",
    "# 回答langchain\n",
    "answer_chain = create_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    ")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8080\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\main.py:587\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m     Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m     \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n\u001b[0;32m    589\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(config\u001b[38;5;241m.\u001b[39muds)  \u001b[38;5;66;03m# pragma: py-win32\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\server.py:61\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: Optional[List[socket\u001b[38;5;241m.\u001b[39msocket]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langserve import add_routes\n",
    "# langchain服务端\n",
    "app = FastAPI()\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    answer_chain,\n",
    "    path=\"/chat\",\n",
    "    input_type=ChatRequest\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
